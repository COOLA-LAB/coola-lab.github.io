<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="COOLA-LAB">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="COOLA-LAB">
<meta property="article:author" content="COOLA-LAB">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>COOLA-LAB</title>
  








<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">COOLA-LAB</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/26/Collaborated-Tasks-driven-Mobile-Charging-and-Scheduling-A-Near-Optimal-Result/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/26/Collaborated-Tasks-driven-Mobile-Charging-and-Scheduling-A-Near-Optimal-Result/" itemprop="url">Collaborated Tasks-driven Mobile Charging and Scheduling: A Near Optimal Result</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-26T21:45:49+08:00">
                2020-11-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这篇论文从任务角度考虑无线传感器网络中的移动充电调度问题，提出一个(1-1/e)4的近似算法。</p>
<h2 id="1-Background"><a href="#1-Background" class="headerlink" title="1. Background"></a>1. Background</h2><p>传感器网络是由部署在特定区域内用于完成感知、采集、计算等一系列传感器节点组成。由于传感器节点的电池容量优先，所以需要定期给传感器充电。传统的方法要么是使用不灵活的静态设备进行充电，要么研究工作只关注移动充电的模式（距离、角度等）或者充电的指标（传输率、延迟等），很少从任务角度来考虑充电调度，导致最终的充电效果不佳。</p>
<p>因此这篇论文从任务级考虑传感器的充电需求，具体的问题场景如下图所示。</p>
<p><img src="/2020/11/26/Collaborated-Tasks-driven-Mobile-Charging-and-Scheduling-A-Near-Optimal-Result/scenario.gif"></p>
<p>在一个区域内，存在一些传感器节点和任务节点，一个传感器可参与多个任务的执行，一个任务也可能同时需要多个传感器协作执行。这篇论文采用移动充电小车给传感器充电，由于小车所携带的能量有限，因此只能挑选一些“重要传感器节点”进行充电，小车从起始位置出发，沿着规划的充电路径，并在完成充电任务后回到源点。</p>
<h2 id="2-System-model-amp-Problem-formulation"><a href="#2-System-model-amp-Problem-formulation" class="headerlink" title="2. System model &amp; Problem formulation"></a>2. System model &amp; Problem formulation</h2><h3 id="System-model"><a href="#System-model" class="headerlink" title="System model"></a>System model</h3><ul>
<li><p>能耗模型</p>
<p>  这篇论文将问题场景中的总能耗分成两个方面：充电小车用于路径的能耗和用于传感器充电的能耗。</p>
<script type="math/tex; mode=display">
  C(X)=\sum_{d\in L^{T\mathrm{S}P}(X)}\alpha \cdot d +\sum_{v_{j}\in X}\beta\cdot(e_{j}-e_{j}^{r})</script><p>  其中$\alpha$表示一个长度单位的能耗，$\beta$表示给传感器充一个单位的能量需要充电小车传输$\beta$个单位能量，$d$是小车路径长度，$e_j-e^r_j$是传感器所需的充电能量。</p>
</li>
<li><p>任务效益模型</p>
<p>  这篇论文定义了传感器对于任务的效益模型：</p>
<script type="math/tex; mode=display">
  u(t_{i},\ v_{j})=w_{ij}p_{ij}</script><p>  $w_{ij}$表示传感器$v_j$和任务$t_i$之间的权值，$p_{ij}$表示$v_j$对$t_i$的能量分配策略。</p>
<p>  此外，每个任务存在一个效益上限$U_i$，所以对于任务$t_i$在传感器集合$X$内的总效益表示为：</p>
<script type="math/tex; mode=display">
  U(X,\displaystyle \ t_{i})=\min\{u_{X}({\it t_i}), U_{i}\}</script><p>  因此，这个传感器网络中的任务总效益为：</p>
<script type="math/tex; mode=display">
  U\left(X,\displaystyle \ P\right)=\sum_{i=1}^{n}U\left(X,\ t_{i}\right)=\sum_{i=1}^{n}\min\left\{u_{X}\left({\it t_i}\right), U_{i}\right\}</script><p>  其中矩阵$P$中的元素表示传感器对于每个任务的分配策略。</p>
</li>
</ul>
<h3 id="Problem-formulation"><a href="#Problem-formulation" class="headerlink" title="Problem formulation"></a>Problem formulation</h3><p>通过上面的定义，最后将问题建模成<strong>CTMC</strong>问题如下形式：</p>
<script type="math/tex; mode=display">
\begin{array}{ll}{\text {  Max }} & {U(X, P)} \\ {\text { s.t. }} & {e_{j}-\sum_{t_{1} \in T} p_{i j} h_{j} \geq 0, \quad \forall v_{j} \in V} \\ { } & {\mathcal{C}(X) \leq E, \quad \forall X \subseteq V}\end{array}</script><p>该问题可分成两个子问题考虑：”Budget maximum coverage problem” 和 “Traveling salesman problem”，根据已有研究表明，这两个子问题都是NP-hard，因此原问题也是一个NP-hard问题。</p>
<h2 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3. Approach"></a>3. Approach</h2><p>这篇论文通过构造一个替代函数 $H(X)$ 来近似原目标函数U(X,P)，具体定义如下：</p>
<script type="math/tex; mode=display">
H_{X}(t_{i})=U(X,t_{i}) \\
H(X)=U(X,P^{'})</script><p>其中$P^{‘}$表示近似的能量分配策略，具体的策略是采用贪心思想：对于每一个传感器，将能量按照权重$w_{ij}$比例分配给对应的任务。</p>
<p>因为原问题还涉及到充电小车的路径规划问题可规约到经典的TSP问题，所以这里又采用了最邻近算法<a href="https://link.springer.com/chapter/10.1007%2F978-1-4020-9688-4_3" target="_blank" rel="noopener"><sup>1</sup></a>来近似最优解，即用$\hat C(X)$近似$C(X)$。</p>
<p>根据证明可发现，函数$H(X)$满足子模函数的性质，同时也具有非负性和单调性。因此这篇论文根据2016年的相关工作<a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11911" target="_blank" rel="noopener"><sup>2</sup></a>基础，采用贪心的近似算法，具体的算法如下：</p>
<p><img src="/2020/11/26/Collaborated-Tasks-driven-Mobile-Charging-and-Scheduling-A-Near-Optimal-Result/RC.gif" alt></p>
<p>RC-ratio贪心思想主要体现在每次选择的传感器节点都是当前使 <script type="math/tex">\frac {H(X_{j-1} \cup \{v\})-H(X_{j-1})}{\hat C(X_{j-1} \cup \{v\})-\hat C(X_{j-1})}</script> 值最大的那个节点。</p>
<h2 id="4-Theoretical-analysis"><a href="#4-Theoretical-analysis" class="headerlink" title="4. Theoretical analysis"></a>4. Theoretical analysis</h2><p>对于替代函数$H(X)=U(X,P^{‘})$可以证明出是最优解$U(X,P)$的1/2。具体证明过程如图中的例子所示。</p>
<p><img src="/2020/11/26/Collaborated-Tasks-driven-Mobile-Charging-and-Scheduling-A-Near-Optimal-Result/example.gif" alt></p>
<p>而算法RC-ratio已经在[2]中被证明是存在一个(1-1/e)/2的近似，因此乘上函数$H(X)$的近似，最终的算法得到(1-1/e)/4的近似比。</p>
<h2 id="5-Evaluation-amp-Experiment"><a href="#5-Evaluation-amp-Experiment" class="headerlink" title="5. Evaluation &amp; Experiment"></a>5. Evaluation &amp; Experiment</h2><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p><img src="/2020/11/26/Collaborated-Tasks-driven-Mobile-Charging-and-Scheduling-A-Near-Optimal-Result/overall task utility.gif"><br><img src="/2020/11/26/Collaborated-Tasks-driven-Mobile-Charging-and-Scheduling-A-Near-Optimal-Result/energy utilization efficience.gif"></p>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><ul>
<li><p>Indoor</p>
<p>  <img src="/2020/11/26/Collaborated-Tasks-driven-Mobile-Charging-and-Scheduling-A-Near-Optimal-Result/Indoor office room.gif" alt></p>
</li>
</ul>
<p><img src="/2020/11/26/Collaborated-Tasks-driven-Mobile-Charging-and-Scheduling-A-Near-Optimal-Result/Overall task utility for indoor scenario.gif" alt="Overall task utility for indoor scenario" style="zoom:50%;"></p>
<ul>
<li>Outdoor</li>
</ul>
<p><img src="/2020/11/26/Collaborated-Tasks-driven-Mobile-Charging-and-Scheduling-A-Near-Optimal-Result/Outdoor soccer field.gif" alt></p>
<p><img src="/2020/11/26/Collaborated-Tasks-driven-Mobile-Charging-and-Scheduling-A-Near-Optimal-Result/Overall task utility for outdoor scenario.gif" alt="Overall task utility for outdoor scenario" style="zoom:50%;"></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li>[1] <a href="https://link.springer.com/chapter/10.1007%2F978-1-4020-9688-4_3" target="_blank" rel="noopener">D. J. Rosenkrantz et al., “An analysis of several heuristics for the traveling salesman problem,” in Fundamental       Problems in Computing.Springer, 2009, pp. 45–69</a>.</li>
<li>[2] <a href="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11911" target="_blank" rel="noopener">H. Zhang et al., “Submodular optimization with routing constraints.” in AAAI, vol. 16, 2016, pp. 819–826.</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/11/11/RouteNet-Leveraging-Graph-Neural-Networks-for-Network-Modeling-and-Optimization-in-SDN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/11/11/RouteNet-Leveraging-Graph-Neural-Networks-for-Network-Modeling-and-Optimization-in-SDN/" itemprop="url">RouteNet: Leveraging Graph Neural Networks for Network Modeling and Optimization in SDN </a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-11T08:43:27+08:00">
                2020-11-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这篇论文提出了一种叫做RouteNet的方法，这是一种基于MPNN的新型网络模型，用于预测某种场景下的网络的特征矩阵，该模型能够了解拓扑，路由和输入流量之间的复杂关系，从而可以准确估算每个源/目标每个数据包的KPI指标。</p>
<h2 id="1-问题描述"><a href="#1-问题描述" class="headerlink" title="1. 问题描述"></a>1. 问题描述</h2><h3 id="一些相关概念"><a href="#一些相关概念" class="headerlink" title="一些相关概念"></a>一些相关概念</h3><ul>
<li>KPI—(Key performance Indicator)<ul>
<li>时延（packet发送和接受的时间差），抖动（相邻packet时延的差值），丢包率（一段时间内丢失的packet比例）等一些描述网络性能的指标，具体可以由一个邻接矩阵矩阵来表示每个节点对之间的一段时间内的平均表现。</li>
</ul>
</li>
<li>网络优化—(Network optimization)<ul>
<li>进行一系列操作使得网络的性能提升</li>
<li>网络优化可以分为两部分循环迭代：1）通过网络模型获取量化的网络性能指标，2）将量化的指标输入优化器获得结果</li>
</ul>
</li>
<li>网络模型—(Network modeling)<ul>
<li>网络优化首先要能度量网络的性能（“<em>we can only optimize what we can model.</em> ”），网络模型就是负责量化网络的目前性能</li>
<li>主流的做法有两个：数学模型（主要是基于排队论的基础—特点是精度低但是速度快）、仿真器（能获得精确的结果，但是仿真速度比较慢）</li>
</ul>
</li>
</ul>
<p>在SDN网络结构中<img src="/2020/11/11/RouteNet-Leveraging-Graph-Neural-Networks-for-Network-Modeling-and-Optimization-in-SDN/image-20201110220025370.png" alt="image-20201110220025370">，如图所示可以实时获取全局的网络信息，从而进行优化，以达到相应的优化目标，左边部分为数据面板，右边部分为控制面板，整个网络优化的过程就可以部署在控制面板中。</p>
<h3 id="本文主要解决问题"><a href="#本文主要解决问题" class="headerlink" title="本文主要解决问题"></a>本文主要解决问题</h3><p>文章主要是利用MPNN的思想建立了一个监督学习的神经网络用来预测网络的KPI指标，监督的样本来自仿真器的Ground-Truth，最后训练出来的模型具有应对类似拓扑的泛化能力以及实验证明可靠的精度。</p>
<p>换言之，主要就是用AI代替了传统网络优化过程中的网络KPI量化这一步骤。</p>
<h2 id="2-主要思想"><a href="#2-主要思想" class="headerlink" title="2. 主要思想"></a>2. 主要思想</h2><h3 id="MPNN-Message-Passing-Neural-Network-—通用GCN框架"><a href="#MPNN-Message-Passing-Neural-Network-—通用GCN框架" class="headerlink" title="MPNN(Message-Passing Neural Network)—通用GCN框架"></a>MPNN(Message-Passing Neural Network)—通用GCN框架</h3><p>文章作者表示，方法的主要思想起源于MPNN（Gilmer J, Schoenholz S S, Riley P F, et al. Neural message passing for quantum chemistry[J]. arXiv preprint arXiv:1704.01212, 2017.），这篇文章里主要概括了之前的主要GCN模型，并抽象出一个普适的GCN框架，这个框架将GCN网络分为三部分：</p>
<ol>
<li>Message Passing</li>
<li>State Update</li>
<li>Readout</li>
</ol>
<p>这边结合之前张凯学长的一篇<a href="https://coola-lab.github.io/2020/09/20/Inductive-Representation-Learning-on-Large-Graphs/" target="_blank" rel="noopener">组会报告</a>来解释这个框架，这篇报告的文章提出了一个GraphSAGE的方法，算法过程如下：<img src="/2020/11/11/RouteNet-Leveraging-Graph-Neural-Networks-for-Network-Modeling-and-Optimization-in-SDN/image-20201110224404481.png" alt></p>
<p>算法过程中标出的部分即为三个MPNN的三个步骤。</p>
<p>GCN的贡献就在于能提取出传统网络的拓扑信息，然后进行接下来的分类回归等操作。</p>
<h3 id="RouteNet"><a href="#RouteNet" class="headerlink" title="RouteNet"></a>RouteNet</h3><p>一个packet传输经过若干条链路，所有链路的集合称之为路径，作者的模型基于以下的原则：</p>
<ol>
<li>每个路径的状态取决于所有链路的状态</li>
<li>一个链路的状态取决于所有经过这条链路的路径的状态</li>
</ol>
<h2 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h2><p>算法过程如下图所示：<img src="/2020/11/11/RouteNet-Leveraging-Graph-Neural-Networks-for-Network-Modeling-and-Optimization-in-SDN/image-20201110231404662.png" alt="image-20201110231404662"></p>
<p>首先初始化路径和各个链路的初始状态，接着对每个链路和路径进行Message Passing 操作，最后再进行readout操作，通过反向传播进行学习。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/10/26/SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/10/26/SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines/" itemprop="url">SiamFC++: Towards Robust and Accurate Visual Tracking with Target Estimation Guidelines</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-10-26T20:46:34+08:00">
                2020-10-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文通过分析和比较各类跟踪器在目标状态估计任务上的表现，做出两项主要贡献：提出一套高性能通用跟踪器的设计要点；根据要点设计出SiamFC++跟踪器，同时证明设计要点的有效性。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>跟踪任务可以被分解为分类任务和状态估计任务的组合：第一个任务的目的是通过分类指出目标的粗略位置，第二个任务则旨在估计精确的目标状态。</p>
<p>对于第二项任务，过去的跟踪器采取以下三种方式：以DCF和SiamFC为代表，采用粗暴的多尺度测试；以ATOM为代表，用梯度递增迭代地调整最初的目标框；以及SiamRPN家族——引入RPN思想。在对以上三种类型的方式进行分析和对比以后，本文总结出四点通用跟踪器的设计要点：</p>
<p>G1：Decomposition of classification and state estimation，分开进行分类和状态估计。</p>
<p>G2：Non-ambiguous scoring，无歧义分数，实际上是具有准确对应关系的分类置信分数。</p>
<p>G3：Prior knowledge-free，无先验知识。目标跟踪任务不应当拥有目标的尺度、大小等先验知识。</p>
<p>G4：Estimation quality assessment，对状态估计质量的独立评估。</p>
<h2 id="2-Main-Idea"><a href="#2-Main-Idea" class="headerlink" title="2. Main Idea"></a>2. Main Idea</h2><p>本文首先遵循G1原则做两个分支。</p>
<p><img src="/2020/10/26/SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines/1.jpg" style="zoom:70%;"></p>
<p>分类部分与siamFC基本一致，采取位置作为训练样本——假如响应图中某个点对应的原图位置位于真实的目标范围中，则这个位置属于正样本。</p>
<p>对于回归分支，其设计思路也是针对每一个位置做回归——最后一层预测特征图上的每个位置$(x,y)$对于的输入图像位置$\left(\left\lfloor\frac{s}{2}\right\rfloor+ xs,\left\lfloor\frac{s}{2}\right\rfloor+ ys\right)$到地面真实边界框上下左右四个边的距离定义为一个4维的向量：</p>
<script type="math/tex; mode=display">
t 
∗
 =(l 
∗
 ,t 
∗
 ,r 
∗
 ,b 
∗
 )</script><p>则对该位置的回归任务可以形式化为：</p>
<script type="math/tex; mode=display">
l^{*}=\left(\left\lfloor\frac{s}{2}\right\rfloor+ xs\right)-x_0</script><script type="math/tex; mode=display">
t^{*}=\left(\left\lfloor\frac{s}{2}\right\rfloor+ ys\right)-y_0</script><script type="math/tex; mode=display">
r^{*}=x_1-\left(\left\lfloor\frac{s}{2}\right\rfloor+ xs\right)</script><script type="math/tex; mode=display">
b^{*}=y_1-\left(\left\lfloor\frac{s}{2}\right\rfloor+ ys\right)</script><p>其中$ (x_0,y_0)$和$ (x_1,y_1)$表示与$(x,y)对应的真实边界框$B^{*}$的左上角和右下角。</p>
<p>由于分类分数和回归操作都是针对“位置”这个概念的，也就是操作的区域都是基于该位置的周围一片区域，而不是与人为设定的anchor对应，所以满足非歧义性（G2）。加上没有预定义anchor，所以也没有先验知识的应用，符合（G3）。</p>
<p>接下来完成G4——对估计质量的独立评价：本文通过添加$1 × 1$卷积层来添加简单但有效的质量评估分支,该层输出反映子窗口中心周围的输入像素在跟踪问题中重要性高于其余部分的优先空间得分（Prior Spatial Score，PSS）：</p>
<script type="math/tex; mode=display">
PSS^{*}=\sqrt{\frac {min(l^{*},r^{*})}{max(l^{*},r^{*})}× \frac {min(t^{*},b^{*})}{max(t^{*},b^{*})}}</script><p>通过将PSS与相应的预测分类分数相乘来产生用于最终框选择的分数。这样，远离物体中心的点的权重就会大大下降，从而提高了跟踪精度。</p>
<h2 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3. Experiments"></a>3. Experiments</h2><h3 id="3-1-From-SiamFC-towards-SiamFC"><a href="#3-1-From-SiamFC-towards-SiamFC" class="headerlink" title="3.1 From SiamFC towards SiamFC++"></a>3.1 From SiamFC towards SiamFC++</h3><p><img src="/2020/10/26/SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines/2.jpg"></p>
<h3 id="3-2-Results-on-Several-Benchmarks"><a href="#3-2-Results-on-Several-Benchmarks" class="headerlink" title="3.2 Results on Several Benchmarks"></a>3.2 Results on Several Benchmarks</h3><p><img src="/2020/10/26/SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines/3.jpg" style="zoom:80%;"></p>
<h3 id="3-3-Comparison-with-Trackers-that-Do-not-Apply-Our-Guidelines"><a href="#3-3-Comparison-with-Trackers-that-Do-not-Apply-Our-Guidelines" class="headerlink" title="3.3 Comparison with Trackers that Do not Apply Our Guidelines"></a>3.3 Comparison with Trackers that Do not Apply Our Guidelines</h3><p><img src="/2020/10/26/SiamFC-Towards-Robust-and-Accurate-Visual-Tracking-with-Target-Estimation-Guidelines/4.jpg" style="zoom:80%;"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/" itemprop="url">Self-Supervised Learning of Depth and Motion Under Photometric Inconsistency</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-09-22T19:00:00+08:00">
                2020-09-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在自监督单目深度估计任务中，文章提出了一种改进的框架，通过增加几何约束和尺度一致性约束，解决动态场景下光度不一致等问题，并增强时间图像序列中的尺度一致性，提升了单目深度和相机自运动估计的结果。</p>
<h2 id="1-Previous-work"><a href="#1-Previous-work" class="headerlink" title="1. Previous work"></a>1. Previous work</h2><p>单目深度估计是指通过机器人的单一摄像头获得的一帧或多帧图像来获取深度信息和机器人姿态的方法。在即使定位与地图构建（SLAM）和视觉里程计（VO）中广泛应用。传统的深度估计方法根据图像像素的特征点进行匹配，受噪声影响误差较大，2017年，Google在CVPR上发表论文Unsupervised Learning of Depth and Ego-motion from Video，提出了自监督学习框架，可以对连续的视频序列做单目深度和相机运动的估计任务。</p>
<script type="math/tex; mode=display">
p_s \sim K_s[R_{t\to s}|t_{t\to s}]D_t(p_t)K^{-1}_tp_t</script><p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/1.png" alt="1" style="zoom:50%;"></p>
<p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/2.png" alt="2" style="zoom:50%;"></p>
<p>这样的估计方法使用光度误差做自监督，存在一定的局限性：</p>
<ul>
<li>场景中存在移动物体、非漫反射表面和遮挡时，光度误差损失函数不适用</li>
<li>在单目场景下，单张物体深度估计结果是基于单张图片的相对深度，在连续多帧的估计结果中存在尺度不一致问题</li>
</ul>
<h2 id="2-Main-Idea"><a href="#2-Main-Idea" class="headerlink" title="2. Main Idea"></a>2. Main Idea</h2><ul>
<li><p>沿用原有的框架和光度一致性损失进行深度和姿态的无监督训练</p>
</li>
<li><p>利用相邻图像的稀疏特征匹配和对极几何约束，增强图像帧间的几何一致性</p>
</li>
<li><p>在相邻图像间增加深度一致性约束，减少深度图像估计结果中的噪声</p>
</li>
</ul>
<script type="math/tex; mode=display">
L_{total}=\alpha L_{pixel}+(1-\alpha)L_{SSIM}+\beta L_{smooth}+\gamma_1L{epi}+\gamma_2L_{reproj}+\mu_1L_{depth}+\mu_2L_{multi}</script><h2 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3. Approach"></a>3. Approach</h2><p>本论文利用了自监督单目深度估计的基本框架，并在此基础上进行了改进，在更复杂的场景下优化单目深度估计结果。框架由三部分构成，分别是深度和位姿估计网络（光度一致性误差）、对极几何一致性约束模块和深度一致性约束模块。</p>
<p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/3.png" alt="3"></p>
<h3 id="3-1-Photometric-consistency"><a href="#3-1-Photometric-consistency" class="headerlink" title="3.1 Photometric consistency"></a>3.1 Photometric consistency</h3><p>给定$N$帧连续的单目图像，无监督的深度和自运动估计问题在同时估计目标（中间帧）图像的深度图$D_t$和到$N-1$个源图像的相对姿势$T_{t \to s}=[R_{T \to S}|t_{t \to s}] \in SE(3)$。使用depth CNN和poseCNN进行训练。帧间重建的对应公式为：</p>
<script type="math/tex; mode=display">
p_s \sim K_s[R_{t\to s}|t_{t\to s}]D_t(p_t)K^{-1}_tp_t</script><p>利用帧间相机运动实现源图像（source）到目标图像（target）的重建，计算损失：</p>
<script type="math/tex; mode=display">
L_{pixel}=\frac {1}{|M|}\sum_{\forall p_t \in M}|\hat I^{(s)}_t(p_t|D_t,T_{t \to s})-I_t(p_t)|</script><p>在像素损失（pixel loss）的基础上增加结构相似性损失（SSIM loss）和边缘平滑损失（edge loss），光度一致性损失的计算公式如下：</p>
<script type="math/tex; mode=display">
L_{baseline}=\alpha L_{pixel}+(1-\alpha)L_{SSIM}+\beta L_{smooth}</script><h3 id="3-2-Epipolar-geometric-consistency"><a href="#3-2-Epipolar-geometric-consistency" class="headerlink" title="3.2 Epipolar geometric consistency"></a>3.2 Epipolar geometric consistency</h3><p>上述重建公式需要满足几个假设：</p>
<ul>
<li>建模场景是静态的，没有移动的对象</li>
<li>场景中的表面是朗伯型的</li>
<li>相邻视图之间不存在遮挡</li>
</ul>
<p>由于现实中的绝大多数场景会违背这些假设，所以使用光度一致性损失进行深度估计的结果存在一定的误差。为此，本文提出了一种通过将间接法几何信息注入直接学习的框架来解决此问题的新颖方法。与依赖于密集光度一致性的直接方法不同，视觉SLAM的间接方法基于稀疏的局部描述符，例如SIFT 和ORB 等。局部不变性受比例和光度变化的影响较小，且可以隐式地嵌入到学习框架中。</p>
<p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/5.png" alt="5" style="zoom: 50%;"></p>
<h4 id="3-2-1-Symmetric-epipolar-error"><a href="#3-2-1-Symmetric-epipolar-error" class="headerlink" title="3.2.1 Symmetric epipolar error"></a>3.2.1 Symmetric epipolar error</h4><p>对极几何约束：</p>
<p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/4.png" alt="4" style="zoom: 25%;"></p>
<script type="math/tex; mode=display">
p_2^tK^{-T}t\times RK^{-1}p_1=0</script><p>令本质矩阵$E=t \times R$，$x_1=K^{-1}p_1$，$x_2=K^{-1}p_2$，则对极约束可以化简为：</p>
<script type="math/tex; mode=display">
x_2^TEx_1=0</script><p>在相机针孔模型中，目标图像和源图像之间的特征匹配$S_{t \harr s}={\{}p \harr p^{‘}{\}}$满足对极约束，其中$p$和$p^{‘}$是校准后的图像坐标。利用对极约束，对于相邻两帧图像中多对匹配的特征点$p$ ，$p^{‘}$，计算<strong>symmetric epipolar error</strong>：</p>
<script type="math/tex; mode=display">
L_{epi}(S|R,t)=\sum_{\forall (p,p^{'}) \in S}(\frac {p^{'T}Ep}{\sqrt{(Ep)^2_{(1)}+(Ep)^2_{(2)}}} + \frac {p^{T}Ep^{'}}{\sqrt {(Ep^{'})^2_{(1)}+(Ep^{'})^2_{(2)}}})</script><h4 id="3-2-2-Re-projection-error"><a href="#3-2-2-Re-projection-error" class="headerlink" title="3.2.2 Re-projection error"></a>3.2.2 Re-projection error</h4><p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/6.png" alt="6" style="zoom: 25%;"></p>
<p>为了用特征匹配进行深度估计结果的优化，可使用估计的深度在一个图像中反投影2D特征以计算3D轨迹，然后将3D轨迹重新投影到另一幅图像以计算重新投影误差。由于特征点$p$的坐标不是整数，因此使用$\hat D_t(p)$在目标深度图像上进行双线性采样。</p>
<script type="math/tex; mode=display">
L_{reproj}(S|R,t,D_t)=\sum _{\forall p \harr p^{'} \in S}||[R|t]\hat D_t(p)p-p^{'}||_2</script><p>设计卷积神经网络的几何误差，将所有匹配项的对极误差和重投影误差降至最低，可模仿传统SLAM中的非线性姿态估计策略。</p>
<h3 id="3-3-Consistent-Depth-Estimation"><a href="#3-3-Consistent-Depth-Estimation" class="headerlink" title="3.3 Consistent Depth Estimation"></a>3.3 Consistent Depth Estimation</h3><p>在深度估计模块中，损失函数从源帧到目标帧成对进行计算，即使位姿估计网络一次输出$N-1$个相对姿势，也无法确定这些相对姿势是否按相同比例对齐。本文提出了运动一致深度估计公式来解决此问题。</p>
<h4 id="3-3-1-Forward-backward-consistency"><a href="#3-3-1-Forward-backward-consistency" class="headerlink" title="3.3.1 Forward-backward consistency"></a>3.3.1 Forward-backward consistency</h4><p>本文提出了单目图像的前后一致性。除了双线性采样像素值外，还估计出当前帧的前向和后向图像的深度图。这个过程产生了两个合成的深度图，它们可以用来约束目标图像深度图$\widetilde D^{(s)}_t$的估计。由于在单目估计中深度仅按比例确定，因此在限制深度之前须对深度按比例进行归一化对齐。前后一致性损失计算公式为：</p>
<script type="math/tex; mode=display">
L_{depth}=\frac{1}{|M|}\sum _{\forall p\in M}|\frac{mean(D_t·M)}{mean(\widetilde D^{(s)}_t·M)}·\widetilde D^{(s)}_t(p)-D_t(p)|</script><h4 id="3-3-2-Multi-view-consistency"><a href="#3-3-2-Multi-view-consistency" class="headerlink" title="3.3.2 Multi-view consistency"></a>3.3.2 Multi-view consistency</h4><p>上述损失函数的设计都是基于单帧进行深度估计，为了增强长视频序列的多帧尺度一致性，本文提出了多视角一致性损失，该损失以目标图像（中间帧图像）为尺度对齐的纽带，惩罚项计算了前向深度和后向深度的不一致损失。</p>
<p>形式上，给定连续图像$(I_1,I_2,I_3)$，$I_2$为目标图像，对应的深度估计结果为$(D_1,D_2,D_3)$，位姿估计结果为$(T_{2 \to 1},T_{2 \to 3})$，归一化深度图计算公式为$\overline D_1=s_{12}·D_1$，$I_1$到$I_3$的位姿变化为$T_{1 \to 3}=T_{2 \to 1}^{-1}·T_{2 \to 3}$。多视图损失将深度一致性项和光度一致性项最小化为：</p>
<script type="math/tex; mode=display">
L_{multi}=\alpha L_{pixel}(I_1,\widetilde I^{(3)}_1)+(1-\alpha)L_{SSIM}(I_1,\widetilde I^{(3)}_1)+\frac{1}{|M_{13}|}\sum_{\forall p\in M_{13}}|\overline D_1(p)-\overline D^{(3)}_1(p)|</script><p>$\widetilde I^{(3)}_1$和$\overline D^{(3)}_1$是给定$\overline D_3$ 和$T_{1 \to 3}$之后通过计算得到的重建图像。$L_{multi}$更优于成对损失$L_{pixel}$，$L_{SSIM}$，$L_{epi}$和$L_{depth}$，它利用了链式姿态计算，将两个相对姿势调整到相同的比例，通过对齐多对连续图像来促进增量定位，优化了单目SLAM的结果。</p>
<h2 id="4-Experiment"><a href="#4-Experiment" class="headerlink" title="4. Experiment"></a>4. Experiment</h2><h3 id="4-1-Depth-Estimation"><a href="#4-1-Depth-Estimation" class="headerlink" title="4.1 Depth Estimation"></a>4.1 Depth Estimation</h3><p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/7.png" alt="7"></p>
<p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/8.png" alt="8"></p>
<h3 id="4-2-Pose-Estimation"><a href="#4-2-Pose-Estimation" class="headerlink" title="4.2 Pose Estimation"></a>4.2 Pose Estimation</h3><p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/9.png" alt="9" style="zoom:33%;"></p>
<p><img src="/2020/09/22/Self-Supervised-Learning-of-Depth-and-Motion-Under-Photometric-Inconsistency/10.png" alt="10" style="zoom:30%;"></p>
<h2 id="5-Reference"><a href="#5-Reference" class="headerlink" title="5. Reference"></a>5. Reference</h2><p><a href="https://arxiv.org/abs/1909.09115v1" target="_blank" rel="noopener">https://arxiv.org/abs/1909.09115v1</a></p>
<p><a href="https://arxiv.org/abs/1704.07813" target="_blank" rel="noopener">https://arxiv.org/abs/1704.07813</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/09/20/Inductive-Representation-Learning-on-Large-Graphs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/20/Inductive-Representation-Learning-on-Large-Graphs/" itemprop="url">Inductive Representation Learning on Large Graphs</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-09-20T17:30:08+08:00">
                2020-09-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-问题定位"><a href="#1-问题定位" class="headerlink" title="1 问题定位"></a>1 问题定位</h2><p>在大规模graph上学到的节点<strong>低维embedding</strong>，在很多预测任务中非常有用，如内容推荐、节点分类等 。但是现在大多数方法都是transductive（直推式）学习， 不能直接泛化到未知节点。这些方法是在一个固定的graph上直接学习每个节点的embedding，但是大多情况graph是会演化的，当网络结构改变以及新节点的出现，直推式学习需要重新训练，很难落地在需要快速生成unseen节点embedding的机器学习系统上。本文<strong>提出inductive（归纳式）学习框架—GraphSAGE(Graph SAmple and aggreGatE)，通过训练多个聚合邻居节点特征的function，将GCN扩展成归纳学习任务，从而对unseen节点起到泛化作用</strong>。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><p>大部分学习节点embedding的方法有以下三种：</p>
<p>一、基于因子分解的embedding生成方法：近年来有许多使用随机游走统计和基于矩阵分解来生成节点低维embedding的方法，如基于spectral clustering（谱聚类）的方法以及PageRank算法。因为这些embedding算法直接为单个节点训练节点embedding，所以它们本质上是transductive的（即不能泛化到unseen节点），而且需要expensive的额外训练(例如，重新通过随机梯度下降)来对新节点进行预测。</p>
<p>二、图上的监督学习：</p>
<p>除了节点嵌入方法之外，还有大量关于对图结构数据进行监督学习的paper。这包括各种基于核的方法，其中图的特征向量是从不同的graph kernel中派生出来的。本文在概念上受到了这些算法的启发。然而，前面的这些方法试图对整个图(或子图)进行分类，而这篇paper的重点是为单个节点生成有用的embedding。</p>
<p>三、图卷积网络：</p>
<p>近年来，人们提出了几种用于图学习的卷积神经网络结构。这些方法中的大多数不能推广到大规模的图，或者是为图分类而设计的。然而，本文的方法与Kipf等人引入的图卷积网络(GCN)密切相关。原来的GCN算法是为transductive设置下的半监督学习而设计的，该算法要求在训练过程中全图Laplacian矩阵是已知的，本文可以看作是GCN框架对inductive设置的扩展。</p>
<h2 id="3-文章工作"><a href="#3-文章工作" class="headerlink" title="3 文章工作"></a>3 文章工作</h2><p>本文提出GraphSAGE框架的核心是如何聚合节点邻居特征信息，本章先<strong>介绍GraphSAGE前向传播过程</strong>（生成节点embedding），<strong>不同的聚合函数</strong>设定；然后介绍<strong>无监督和有监督的损失函数</strong>以及<strong>参数学习。</strong></p>
<h3 id="3-1-前向传播"><a href="#3-1-前向传播" class="headerlink" title="3.1 前向传播"></a>3.1 前向传播</h3><p><strong>a. 可视化例子：</strong>下图是GraphSAGE 生成目标节点（红色）embededing并供下游任务预测的过程：</p>
<p><img src="/2020/09/20/Inductive-Representation-Learning-on-Large-Graphs/v2-7e94e024910274eee88ab3947fd3dff4_1440w.jpg" alt="image-20200924141919374"></p>
<ol>
<li>先对邻居随机采样，降低计算复杂度（图中一跳邻居采样数=3，二跳邻居采样数=5）</li>
<li>生成目标节点emebedding：先聚合2跳邻居特征，生成一跳邻居embedding，再聚合一跳邻居embedding，生成目标节点embedding，从而获得二跳邻居信息。</li>
<li>将embedding作为全连接层的输入，预测目标节点的标签。</li>
</ol>
<p><strong>b. 伪代码:</strong></p>
<p><img src="/2020/09/20/Inductive-Representation-Learning-on-Large-Graphs/image-20200924141947029.png" alt="image-20200924141947029"></p>
<p>4-5行是核心代码，介绍卷积层操作：聚合与节点v相连的邻居（采样）k-1层的embedding，得到第k层邻居聚合特征  $h^k_{N(v)}$，与节点v第k-1层embedding $h^{k-1}_v$拼接，并通过全连接层转换，得到节点v在第k层的embedding  $h^k_{v}$。</p>
<h3 id="3-2-聚合函数"><a href="#3-2-聚合函数" class="headerlink" title="3.2 聚合函数"></a><strong>3.2 聚合函数</strong></h3><p>伪代码第5行可以使用不同聚合函数，本小节介绍五种满足排序不变量的聚合函数：平均、GCN归纳式、LSTM、pooling聚合器。（因为邻居没有顺序，聚合函数需要满足排序不变量的特性，即输入顺序不会影响函数结果）</p>
<p><strong>a.平均聚合：</strong>先对邻居embedding中每个维度取平均，然后与目标节点embedding拼接后进行非线性转换。</p>
<script type="math/tex; mode=display">h^k_{N(v)} = mean({h^{k-1}_u,u\in N(v)})</script><script type="math/tex; mode=display">h^k_v = \sigma(W^k . CONCAT(h^{k-1}_v,h^k_{N(u)}))</script><p><strong>b. 归纳式聚合：</strong>直接对目标节点和所有邻居emebdding中每个维度取平均（替换伪代码中第5、6行），后再非线性转换：</p>
<script type="math/tex; mode=display">h^k_v = \sigma(W^k . mean(\{h^{k-1}_v\} \cup \{h^{k-1}_u,\forall u \in N(v)\}))</script><p><strong>c. LSTM聚合：</strong>LSTM函数不符合“排序不变量”的性质，需要先对邻居随机排序，然后将随机的邻居序列embedding  $\{x_t,t\in N(v)\}$作为LSTM输入。</p>
<p><strong>d. Pooling聚合器:</strong>先对每个邻居节点上一层embedding进行非线性转换（等价单个全连接层，每一维度代表在某方面的表示（如信用情况）），再按维度应用 max/mean pooling，捕获邻居集上在某方面的突出的／综合的表现 以此表示目标节点embedding。</p>
<script type="math/tex; mode=display">h^k_{N(v)} = max(\{\sigma(W_{pool}{h^k_{u_i}+b,\forall u_i\in N(v)})</script><script type="math/tex; mode=display">h^k_v = \sigma(W^k . CONCAT(h^{k-1}_v,h^k_{N(u)}))</script><h3 id="3-3-无监督和有监督损失设定"><a href="#3-3-无监督和有监督损失设定" class="headerlink" title="3.3 无监督和有监督损失设定"></a><strong>3.3 无监督和有监督损失设定</strong></h3><p>损失函数根据具体应用情况，可以使用<strong>基于图的无监督损失</strong>和<strong>有监督损失</strong>。</p>
<p><strong>a. 基于图的无监督损失：</strong>希望节点u与“邻居”v的embedding也相似（对应公式第一项），而与“没有交集”的节点$v_n$不相似（对应公式第二项)。</p>
<script type="math/tex; mode=display">J_G(z_u) = -log(\sigma(z^T_uz_v)) - Q.E_{v_n \sim P_n(v)}log(\sigma(-z^T_uz_{v_n}))</script><ul>
<li>$z_u$为节点u通过GraphSAGE生成的embedding。</li>
<li>节点v是节点u随机游走访达“邻居”。</li>
<li>$v_n \sim P_n(u)$表示负采样：节点 $v_n$是从节点u的负采样分布 $P_n$ 采样的，Q为采样样本数。</li>
<li>embedding之间相似度通过向量点积计算得到</li>
</ul>
<p><strong>b. 有监督损失：</strong>无监督损失函数的设定来学习节点embedding 可以供下游多个任务使用，若仅使用在特定某个任务上，则可以替代上述损失函数符合特定任务目标，如交叉熵。</p>
<h3 id="3-4-参数学习"><a href="#3-4-参数学习" class="headerlink" title="3.4 参数学习"></a>3.4 参数学习</h3><p>通过前向传播得到节点u的embedding  $z_u$ ,然后梯度下降（实现使用Adam优化器） <strong>进行反向</strong>传播优化参数 $W^k$ 和聚合函数内参数。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/09/13/Dynamic-Risk-Density-for-Autonomous-Navigation-in-Cluttered-Environments-without-Object-Detection/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/13/Dynamic-Risk-Density-for-Autonomous-Navigation-in-Cluttered-Environments-without-Object-Detection/" itemprop="url">Dynamic Risk Density for Autonomous Navigation in Cluttered Environments without Object Detection</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-09-13T19:00:00+08:00">
                2020-09-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这篇论文提出了一种叫做“动态风险密度”（Dynamic Risk Density）的方法，用于解决在没有目标检测的杂乱环境下进行导航的问题。</p>
<h2 id="1-Previous-work"><a href="#1-Previous-work" class="headerlink" title="1. Previous work"></a>1. Previous work</h2><p><img src="/2020/09/13/Dynamic-Risk-Density-for-Autonomous-Navigation-in-Cluttered-Environments-without-Object-Detection/1.jpg" style="zoom:33%;"></p>
<p>在 Navigating Congested Environments with Risk Level Sets 这篇论文中，作者提出了占据风险代价函数：</p>
<script type="math/tex; mode=display">H(q,x,\dot{x})=\sum^{n}_{i=1}{\frac{\exp(-(q-x_i)^T\Omega(q-x_i))}{1+\exp(-\alpha\dot{x_i}^T(q-x_i))}},\Omega=diag\{\frac{1}{\sigma^2_x},\frac{1}{\sigma^2_y}\} \tag{1}</script><p>其中，</p>
<ul>
<li>$q$：表示二维平面内的任意一个点；</li>
<li>$x$：表示二维平面内所有障碍物的位置，例如图(a)中的四个点；</li>
<li>$\dot{x}$：表示二维平面内所有障碍物的速度，例如图(a)中四个点上的箭头。</li>
</ul>
<p>通过计算 $H$，我们可以得到平面内每个点的占据风险代价，可以将其看成是该点在下一个时刻会出现障碍物的概率。在导航的过程中，为了不发生碰撞，自然要避免经过 $H$ 值大的点。</p>
<p>$H$ 是由一个高斯峰 $\exp(-(q-x_i)^T\Omega(q-x_i))$ 乘上一个 logistic 函数 $\frac{1}{1+\exp(-\alpha\dot{x_i}^T(q-x_i))}$ 构成。很显然，离障碍物越近的点 $H$ 值会越大，这由高斯峰来进行描述，logistic 函数则是对这个高斯峰在速度方向上进行了偏移，如图(b)所示。</p>
<p>在导航的过程中，给定一个阈值 $H_p$，只要确保只在 $H\leq H_p$ 的区域内行动就能避免发生碰撞。</p>
<h2 id="2-Main-Idea"><a href="#2-Main-Idea" class="headerlink" title="2. Main Idea"></a>2. Main Idea</h2><p>在 Previous work 提到的方法中，我们假设已知了环境中所有障碍物的位置和速度，但在现实场景下，这往往是很难得到的。因此，在 Dynamic Risk Density for Autonomous Navigation in Cluttered Environments without Object Detection 这篇论文中，作者对其进行了改进，分别用“占据密度”(occupancy density)和“速度场”(velocity flow field)代替了障碍物的位置和速度。提出了动态风险密度：</p>
<script type="math/tex; mode=display">H_\rho(q,t,\rho,V)=\frac{\rho(q,t)}{1+\exp(\alpha\nabla\rho(q,t)\cdot V(q,t))} \tag{2}</script><p>其中，</p>
<ul>
<li>$\rho$：表示占据密度；</li>
<li>$V$：表示速度场。</li>
</ul>
<p>$H_\rho$ 和 $H$ 的意义类似。</p>
<p><img src="/2020/09/13/Dynamic-Risk-Density-for-Autonomous-Navigation-in-Cluttered-Environments-without-Object-Detection/2.jpg" style="zoom:33%;"></p>
<p>通过仿真可以看出，由这两种方法得到的轮廓线非常相似（左图是计算 $H$ 值得到的，右图是计算 $H_\rho$ 值得到的）。</p>
<h2 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3. Approach"></a>3. Approach</h2><p>在这篇论文中，通过使用一个二维激光扫描仪来构建环境的占据网格而得到占据密度。</p>
<p>对于如何计算速度场，论文中提出了两种方法，分别是基于<strong>聚类</strong>的方法和基于<strong>密度流</strong>的方法。</p>
<h3 id="3-1-Velocity-Field-Estimation-from-Density-Flow"><a href="#3-1-Velocity-Field-Estimation-from-Density-Flow" class="headerlink" title="3.1 Velocity Field Estimation from Density Flow"></a>3.1 Velocity Field Estimation from Density Flow</h3><p>基于聚类的方法，使用 k-means 算法对占据网格进行聚类，并计算聚类中心的速度，再通过 Voronoi 分布将聚类中心的速度映射到所有的点上。</p>
<p><img src="/2020/09/13/Dynamic-Risk-Density-for-Autonomous-Navigation-in-Cluttered-Environments-without-Object-Detection/3.jpg" style="zoom:33%;"></p>
<h3 id="3-2-Velocity-Field-Estimation-from-Density-Flow"><a href="#3-2-Velocity-Field-Estimation-from-Density-Flow" class="headerlink" title="3.2 Velocity Field Estimation from Density Flow"></a>3.2 Velocity Field Estimation from Density Flow</h3><p>基于密度流的方法与计算机视觉中的光流法类似。</p>
<p>在这里引入了两个假设：</p>
<ol>
<li>密度不变性，即同一个点在经过 $\Delta t$ 时间移动了 $\Delta x$， $\Delta y$ 之后，密度没有发生改变：</li>
</ol>
<script type="math/tex; mode=display">\rho^t(x,y)=\rho(x,y,t)=\rho(x+\Delta x,y+\Delta y,t+\Delta t) \tag{3}</script><ol>
<li>对于 $\rho(x+\Delta x,y+\Delta y,t+\Delta t)$ 可以进行一阶泰勒展开：</li>
</ol>
<script type="math/tex; mode=display">\rho(x+\Delta x,y+\Delta y,t+\Delta t)=\rho(x,y,t)+\frac{\partial\rho}{\partial x}\Delta x+\frac{\partial\rho}{\partial y}\Delta y+\frac{\partial\rho}{\partial t}\Delta t+H.O.T. \tag{4}</script><p>结合(3)(4)可以得到：</p>
<script type="math/tex; mode=display">\frac{\partial\rho}{\partial x}\frac{\Delta x}{\Delta t} +\frac{\partial\rho}{\partial y}\frac{\Delta y}{\Delta t}+\frac{\partial\rho}{\partial t}=0 \tag{5}</script><p>对(5)进行变量替换：</p>
<script type="math/tex; mode=display">u=\frac{\Delta x}{\Delta t},v=\frac{\Delta y}{\Delta t}</script><script type="math/tex; mode=display">I_x=\frac{\partial\rho}{\partial x},I_y=\frac{\partial\rho}{\partial y},I_t=\frac{\partial\rho}{\partial t}</script><p>化简可得：</p>
<script type="math/tex; mode=display">\left[\begin{matrix} I_x&I_y \end{matrix}\right] \left[\begin{matrix} u\\v \end{matrix}\right] = -I_t \tag{6}</script><p>这是一个二元一次方程，无法进行求解，因此，必须引入额外的约束来计算 u,v。我们假设某一个窗口内的点具有相同的运动。</p>
<p>考虑一个大小为 $w*w$ 大小的窗口，它含有 $w^2$ 数量的像素。由于该窗口内像素具有 同样的运动，因此我们共有 $w^2$ 个方程： </p>
<script type="math/tex; mode=display">\left[\begin{matrix} I_x&I_y \end{matrix}\right]_k \left[\begin{matrix} u\\v \end{matrix}\right] = -I_{tk}, k=1,2,...,w^2 \tag{7}</script><p>记：</p>
<script type="math/tex; mode=display">A=\left[\begin{matrix}[I_x,I_y]_1\\ \vdots\\ [I_x,I_y]_k\end{matrix}\right],b=\left[\begin{matrix}I_{t1}\\ \vdots\\ I_{tk}\end{matrix}\right]</script><p>于是整个方程为：</p>
<script type="math/tex; mode=display">A\left[\begin{matrix}u\\v\end{matrix}\right]=-b \tag{8}</script><p>利用最小二乘法可以求得：</p>
<script type="math/tex; mode=display">\left[\begin{matrix}u\\v\end{matrix}\right]^*=-(A^TA)^{-1}A^Tb \tag{9}</script><h3 id="3-3-Navigation-Algorithm"><a href="#3-3-Navigation-Algorithm" class="headerlink" title="3.3 Navigation Algorithm"></a>3.3 Navigation Algorithm</h3><p><img src="/2020/09/13/Dynamic-Risk-Density-for-Autonomous-Navigation-in-Cluttered-Environments-without-Object-Detection/4.jpg" style="zoom:33%;"></p>
<p>Experiment：<a href="https://www.youtube.com/watch?v=uXry23LxpWw" target="_blank" rel="noopener">https://www.youtube.com/watch?v=uXry23LxpWw</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/09/10/Learning-RoI-Transformer-for-Detecting-Oriented-Objects-in-Aerial-Images/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/10/Learning-RoI-Transformer-for-Detecting-Oriented-Objects-in-Aerial-Images/" itemprop="url">Learning RoI Transformer for Detecting Oriented Objects in Aerial Images</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-09-10T13:22:00+08:00">
                2020-09-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-问题定义"><a href="#1-问题定义" class="headerlink" title="1.问题定义"></a>1.问题定义</h2><p>目标检测是计算机视觉领域的一个基本问题, 其目标是在场景中快速、精确地定位、识别特定目标，从而为很多计算机视觉应用场景提供重要的信息基础。近年来，该领域随着深度学习技术的快速发展取得了显著的突破，然而由于俯瞰视角、高度复杂的背景以及物体多样的外观，航拍图像中的物体检测是计算机视觉中一项具有挑战性的任务。特别是当在航拍图像中检测密集的物体时，依赖于水平候选框（Horizontal Region of Interest ，HRoI)的普通目标检测的方法经常引入候选框和物体之间的不匹配，并最终导致物体分类和定位的误差。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2.相关工作"></a>2.相关工作</h2><p>由于基于HRoI的方法总是从水平的特征图中提取特征并进行边框回归，因此对于旋转的物体并不能达到很好的精度。在此之前也有一些方法使用Rotated anchor 来解决这个问题，但是它们的设计总是使 anchor 的数量倍增并且显著增加了计算复杂性。</p>
<h2 id="3-本文工作"><a href="#3-本文工作" class="headerlink" title="3.本文工作"></a>3.本文工作</h2><p>本文针对旋转的物体仍然使用Horizontal anchors，在有有向边界框标注的前提下，提出新的方法 RoI Transformer。核心思想是先生成HRoI,再利用空间特征信息学习到偏转角度参数，最后作相应的调整以达到旋转不变性，同时解决了大量的anchor设计以及特征不匹配的问题。</p>
<p>本文基于 Light Head R-CNN 进行改进，提出了两个新的模块：</p>
<ul>
<li><p>RRoI Learner ：此模块会将 HRoI 传递给 PS RoI Align 层， 再接一个轻量的维度为5的全连接层，回归 Rotated Ground Truths 相对于HRoIs的偏移, 通过解码器将 HRoI 和偏移量作为输入并输出的 RRoI:</p>
<script type="math/tex; mode=display">
  (t_x,t_y,t_w,t_h,t_\theta）</script></li>
<li><p>RRoI Transformer：经过RRoI Learner学习到了旋转参数后，我么在这里就能扭曲特征图以保持深度特征的旋转不变性。RRoI Learner 和 RRoI 变形的组合构成了 RoI Transformer ，然后使用来自 RoI Transformer 的几何鲁棒合并的特征进行 RRoI 的分类和回归。</p>
</li>
</ul>
<p><img src="/2020/09/10/Learning-RoI-Transformer-for-Detecting-Oriented-Objects-in-Aerial-Images/architecture.png" alt></p>
<p>由于我们想要实现物体的旋转不变性特征，而通用检测的损失函数在这里就会出现问题了，在不同角度的情况下，两个旋转幅度相同的RRoI和RGT却会出现不同的横纵坐标偏移量。于是作者基于局部坐标系，对损失函数进行了修改：</p>
<script type="math/tex; mode=display">
t^*_x=1/w_r((x^*-x_r)cos\theta_r+(y^*-y_r)sin\theta_r),</script><script type="math/tex; mode=display">
t^*_y=1/h_r((y^*-y_r)cos\theta_r-(x^*-x_r)sin\theta_r),</script><script type="math/tex; mode=display">
t^*_\theta=1/2\pi((\theta^*-\theta_r) mod2 \pi)</script><p>这样我们通过满足物体相对偏移的旋转不变性来实现了物体特征的旋转不变性，并学习到旋转角度。</p>
<p>本文通过 RoI Transformer 模块模拟几何变换并解决区域特征和物体的不匹配问题，为具有挑战性的 DOTA 和 HRSC 数据集的旋转物体检测带来了显著的改进。对于旋转物体，在有有向边界框标注的情况下， RoI Transformer模块更为合理。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/30/AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/08/30/AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition/" itemprop="url">AdaFrame - Adaptive Frame Selection for Fast Video Recognition</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-08-30T16:57:38+08:00">
                2020-08-30
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-Introduce"><a href="#1-Introduce" class="headerlink" title="1 Introduce"></a>1 Introduce</h2><p>本文中的场景为视频的快速描述，问题的输入是一段视频序列，输出是视频的类别或标签。目标为efficient video recognition。这类问题一般会对视频均匀采样取帧，这默认了视频的信息是均匀分布的，但实际上并非如此，均匀采样会取到无用的背景帧或冗余的信息帧。</p>
<p><img src="/2020/08/30/AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition/1.png" alt></p>
<p>分类任务的类别不同，需要的帧数通常也不同，静态物体的识别一般只需要一帧，而复杂的动作识别需要多帧，同类别间也有可能需要不同帧数。 </p>
<p><img src="/2020/08/30/AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition/2.png" alt></p>
<h2 id="2-Approach"><a href="#2-Approach" class="headerlink" title="2 Approach"></a>2 Approach</h2><p><img src="/2020/08/30/AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition/3.png" alt></p>
<p>此图为网络的整体结构图。memory-agumented LSTM的输入是一个视频帧序列，在每个step中，将</p>
<ol>
<li>当前帧的特征vt</li>
<li>前一个step的状态h(t-1)、c(t-1)</li>
<li>从golbal memory中得到的Global context ut</li>
</ol>
<p>输入到LSTM中，得到当前step的隐藏状态h(t)，之后h(t)用来计算</p>
<ol>
<li>通过Prediction Network得到prediction向量st，用来计算Reward</li>
<li>通过Selection Network得到action a，用来决定下一步看哪一帧图像</li>
<li>通过Utility Network得到utility vt，用来判断继续看下一帧的预期收益</li>
</ol>
<h3 id="2-1-Memory-agumented-LSTM"><a href="#2-1-Memory-agumented-LSTM" class="headerlink" title="2.1 Memory-agumented LSTM"></a>2.1 Memory-agumented LSTM</h3><script type="math/tex; mode=display">
h_{t},c_{t}=LSTM([v_{t},u_{t}],h_{t-1},c_{t-1})</script><p>帧特征 vt 跟 全局上下文特征 ut 拼接起来作为当前LSTM单元的输入。</p>
<h4 id="Global-memory"><a href="#Global-memory" class="headerlink" title="Global memory"></a>Global memory</h4><p>global memory部分用于提供上下文信息，它的输入是经过空间和时间上降采样取到的帧的集合，用一个lightweight CNN分别提取他们的特征.</p>
<script type="math/tex; mode=display">
M=[v_1^s,v_1^s,...,v_{T_d}^s]</script><p>M为特征集合，由于只是分别对各个帧使用2D CNN，没有利用到时间信息，因此使用PE（position encoding）的方法来将位置信息嵌入到frames represetations中。</p>
<script type="math/tex; mode=display">
z_{t,j}=(W_hh_{t-1})^TPE(v_j^s)</script><p>使用soft-attention来获取global context information。根据上一个时刻的隐藏状态h(t-1)来给每个frame representation加上attention权值</p>
<script type="math/tex; mode=display">
\beta _t=Softmax(z_t)</script><script type="math/tex; mode=display">
u_t = \beta _t^TM</script><h4 id="Reward-function"><a href="#Reward-function" class="headerlink" title="Reward function"></a>Reward function</h4><script type="math/tex; mode=display">
    r_t=max\{0,m_t - max_{t^, \in[0,t-1]} m_{t^,}\}</script><script type="math/tex; mode=display">
    m_t=s_t^{g^t}-max\{s_t^{c^,}|c^,\ne gt \}</script><p>mt代表预测出的对于ground-true类别的可能性比其他类别可能性最大值之间的差距。优化过程中会增大这个差距。Reward函数激励这个margin比历史margin更大，这样设计reward的目的是衡量这一帧是否有助于增大ground-true class的probability，这个帧是否是有意义的。</p>
<h4 id="Selection-network"><a href="#Selection-network" class="headerlink" title="Selection network"></a>Selection network</h4><p>Selecttion network用来决定下一个step看哪一帧。</p>
<script type="math/tex; mode=display">
f_s(h_t;W_s)=a_t = sigmoid(W_s^Th_t)</script><p>直接用一个全连接层将隐藏状态ht映射到一个value上。然后再加上一个高斯分布采样（方差固定为0.01）。加入高斯分布的目的是在训练过程中加入噪声。在inference阶段不用高斯分布采样，直接用at，乘以总帧数就是下一帧的位置。可以发现下一帧不一定是后面的帧，也可以是前面的帧。</p>
<p>这样训练selection network来最大化期望reward：</p>
<script type="math/tex; mode=display">
J_{sel}(W_s)=E_{l_t~\pi(.|h_t;W_s)[\sum _{t=0}^{T_e}r_t]}</script><h4 id="Utility-network"><a href="#Utility-network" class="headerlink" title="Utility network"></a>Utility network</h4><script type="math/tex; mode=display">
f_u(h_t;W_u)=\hat V_t =W_u^Th</script><p>用一个全连接层ht映射为Vt，代表approximation of expected future rewards，其中expected future rewards可以采用公式计算：</p>
<script type="math/tex; mode=display">
V_t=E_{h_t{t+1}:T_e,a_t:T_e}[\sum_{t=0}^{T_e-t}\gamma^ir_{t+i}]</script><p>通过损失函数来训练：</p>
<script type="math/tex; mode=display">
L_{utl}(W_u)= {1\over 2}||\hat V_t-V_t||_2</script><h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h4><p>综合上述损失函数，最终的目标函数为：</p>
<script type="math/tex; mode=display">
minimize_\Theta L_{cls}+\lambda L_{utl}-\lambda J_{sel}</script><p>前两项可微，可以使用SGD反向传播更新参数即可。</p>
<p>分析第三项的梯度如下：</p>
<script type="math/tex; mode=display">
\nabla _\Theta J_{sel} = E[\sum _{t=0}^{T_e}(R_t-\hat V_t)\nabla _\Theta log\pi _\Theta(.|h_t)]</script><p>vt作为baseline减小方差。Rt代表expected future reward。在一个mini-batch中用Monte-Carlo sampling来拟合。</p>
<h3 id="2-2-Adaptive-Lookahead-Inference"><a href="#2-2-Adaptive-Lookahead-Inference" class="headerlink" title="2.2 Adaptive Lookahead Inference"></a>2.2 Adaptive Lookahead Inference</h3><p>前面讲到的utility network用来预测expected future rewards，看到将来取更多帧所带来的好处。因此根据utility network的输出来决定继续或者停止取帧。在每个时刻都计算vt，并记录vt的最大值，如果有p次当前值比最大值大一个阈值margin，则可停止。</p>
<h2 id="3-Experiment"><a href="#3-Experiment" class="headerlink" title="3 Experiment"></a>3 Experiment</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><ul>
<li><p>FCVID：91, 223 videos from YouTube. 共239个类别，平均167秒。trainset：45, 611 testset：45, 612</p>
</li>
<li><p>ACTIVITYNET：20K videos，共200个类别。平均117秒。train,val,test=10, 024 vs 4, 926 vs 5, 044。用val作为test。</p>
</li>
</ul>
<h3 id="Main-Result"><a href="#Main-Result" class="headerlink" title="Main Result"></a>Main Result</h3><p><img src="/2020/08/30/AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition/4.png" alt></p>
<p><img src="/2020/08/30/AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition/5.png" alt></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/11/Tagoram-Real-Time-Tracking-of-Mobile-RFID-Tags-to-High-Precision-Using-COTS-Devices/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/08/11/Tagoram-Real-Time-Tracking-of-Mobile-RFID-Tags-to-High-Precision-Using-COTS-Devices/" itemprop="url">Tagoram: Real-Time Tracking of Mobile RFID Tags to High Precision Using COTS Devices</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-08-11T21:19:19+08:00">
                2020-08-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="RFID-Technology-Background"><a href="#RFID-Technology-Background" class="headerlink" title="RFID Technology Background"></a>RFID Technology Background</h3><p>RFID 是 Radio Frequency Identification 的缩写，即射频识别。RFID 射频识别是一种非接触式的自动识别技术，其基本原理是利用射频信号和空间耦合（电感或电磁耦合）或雷达反射的传输特性，实现对被识别物体的自动识别并获取相关数据，无须人工干预，可工作于各种恶劣环境。RFID 技术可识别高速运动物体并可同时识别多个标签，操作快捷方便。</p>
<p>最基本的 RFID 系统由三部分组成：</p>
<ol>
<li>阅读器(Reader)：读取或写入标签信息的设备，可设计为手持式或固定式，目前国际上主流RFID设备生产商有Impinj，Alien，Zebra。</li>
<li>天线(Antenna)：在标签和读取器间传递射频信号。</li>
<li>标签(Tag)：由耦合元件及芯片组成，每个标签具有唯一的电子编码，附着在物体上标识目标对象；依据电子标签供电方式的不同，电子标签可以分为有源电子标签(Active tag)、无源电子标签(Passive tag)和半无源电子标签(Sem-passive tag)。有源电子标签内装有电池，无源射频标签没有内装电池，半无源电子标签(Sem-passive tag)部分依靠电池工作。专门生产RFID标签的有SMARTRAC。</li>
</ol>
<p>射频识别系统的工作频率基本上划归三个范围：</p>
<ol>
<li>30kHz-300kHz：低频标签一般为无源标签， 其工作能量通过电感耦合方式从阅读器耦合线圈的辐射近场中获得，阅读距离一般情况下小于 1 米。频标签的典型应用有：动物识别、容器识别、工具识别、电子闭锁防盗等。</li>
<li>3MHz ~ 30MHz：中频标签一般也采用无源设主，其工作能量同低频标签一样，阅读距离一般情况下也小于 1 米。中频标签由于可方便地做成卡状，典型应用包括：电子车票、电子身份证、门禁设备等。</li>
<li>超高频（ 300MHz ~ 3GHz）或微波（ &gt;3GHz）：一般可用频段：433.92MHz， 862(902)~928MHz， 2.45GHz， 5.8GHz。标签与阅读器之间的耦合方式为电磁耦合方式。 阅读距离一般大于 1m，典型情况为 4~6m，最大可达 10m 以上。阅读器天线一般均为定向天线，只有在阅读器天线定向波束范围内的射频标签可被读/写。典型应用包括：移动车辆识别、仓储物流应用等。</li>
</ol>
<p>Tracking RFID Application 使用RFID进行跟踪的应用</p>
<ul>
<li>分析顾客消费习惯</li>
<li>运动员路线追踪</li>
<li>人机互动，肢体判断</li>
<li>机器人判断追踪物品位置</li>
<li>机场行李追踪及排序</li>
</ul>
<hr>
<h3 id="Tagoram-Overview"><a href="#Tagoram-Overview" class="headerlink" title="Tagoram Overview"></a>Tagoram Overview</h3><p><strong>思路来源</strong></p>
<p>考虑使用RFID无源标签的反向散射信号中的相位值θ，θ为读写器发送和接收信号的相位差。Impinj  Reader硬件设备检测到的相位值为一个12位的数，相位分辨率可达 $ 2\pi \div 4096 = 0.0015rad $ , Reader的平均波长为320mm，理论上的检测距离可达0.038mm。</p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_backscatter communication_20200804.jpg" alt="research_rfid_backscatter communication_20200804"></p>
<h4 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h4><ol>
<li>RF相位受热噪声（thermal noise）的影响：在0度到40度的环境下，920 ∼ 926MHz的信道带宽（包含16个通道），RSS从-70 - -30dbm的环境条件下对多个标签测试相位，其满足标准差为0.1的正态分布。如何在不确定的相位信息中得到精确且确定的追踪结果是个问题。</li>
<li>硬件差异会造成额外的相位偏移（diversity term），从而导致标签和读写器的校准无法实现，因为使用不同的硬件需要不同的校准；</li>
<li>环境变化使得相位测量结果复杂。</li>
</ol>
<h4 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h4><p>$ A = \{ A_1, A_2, … , A_M \} $ </p>
<p>天线序列，$M$ : 天线数，$A_m$ : 第 m 根天线</p>
<p>$ \Theta=\begin{pmatrix}<br>\theta_{1,1} &amp; … &amp; \theta_{1,N}\\<br>\vdots &amp; \vdots &amp; \vdots\\<br>\theta_{M,1} &amp; \cdots &amp; \theta_{M,N}\\<br>\end{pmatrix} $ </p>
<p>$ \Theta $ : 测得的相位值，$ \theta_{m,n} $ 表示第m根天线第n轮的检测值，$N$ : 总共做 N轮次收集</p>
<p>检测区域分成为 $ W \times L $ 的grids</p>
<script type="math/tex; mode=display">T=\begin{pmatrix}
t_{1,1} & ... & t_{1,N}\\
\vdots & \vdots & \vdots\\
t_{M,1} & \cdots & t_{M,N}\\
\end{pmatrix}=t_0+\begin{pmatrix}
  \triangle_{1,1} & ... & \triangle_{1,N}\\
  \vdots & \vdots & \vdots\\
  \triangle_{M,1} & \cdots & \triangle_{M,N}\\
\end{pmatrix}\,.</script><p>$ T $ : 测得时间戳，$t_0$: 最小的时间，开始记录的时间，$\triangle_{m,n}$: 记录的时间与开始的时间差</p>
<p>找到$\{f(t_{1,1}),f(t_{1,2}),…,f(t_{M,N})\}$ tag 在随时间变化的标签运动轨迹</p>
<hr>
<h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><h4 id="Movement-with-known-track"><a href="#Movement-with-known-track" class="headerlink" title="Movement with known track"></a>Movement with known track</h4><p>Movement with known track意为the tags move along a known track with a constant speed，此时只需知道标签的初始位置 $ f(t_0) $ 即可，<strong>virtual antenna matrix</strong> 使用虚拟天线阵列</p>
<p>假设标签不动，天线运动，则天线的运动为天线的初始位置与标签运动的反向叠加。</p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_ rfid_tagoram_virtual antenna matrix_20200804.jpg" alt="research_ rfid_tagoram_virtual antenna matrix_20200804"></p>
<p>则用 $ A $ 表示虚拟天线阵列， </p>
<script type="math/tex; mode=display">A=\begin{pmatrix}
A_{1,1} & ... & A_{1,N}\\
\vdots & \vdots & \vdots\\
A_{M,1} & \cdots & A_{M,N}\\
\end{pmatrix}</script><p>有 $ A_{m,n}=A_m-\vec V \times\Delta _{m,n} $</p>
<p>提出建立RF hologram( RF全息图 )</p>
<p><strong>全息图含义</strong>：将平面分成毫米级W ×L的网格，并把图心作为每一个网格的坐标，RF全息图显示的是每一个网格成为初始坐标的可能性。用 $ I $ 表示。</p>
<script type="math/tex; mode=display">I=\begin{pmatrix}
x_{1,1} & ... & x_{1,L}\\
\vdots & \vdots & \vdots\\
x_{W,1} & \cdots & x_{W,L}\\
\end{pmatrix}</script><h5 id="Naive-hologram-朴素全息图"><a href="#Naive-hologram-朴素全息图" class="headerlink" title="Naive hologram 朴素全息图"></a>Naive hologram 朴素全息图</h5><p>$ h(X, A) =\frac{4\pi}{\lambda}|X\text{ }A|\text{ mod } 2\pi $ 理论相位</p>
<p>理论相位表示如果X位置存在标签，从天线A发出并在X方格位置反射的理论相位值。</p>
<script type="math/tex; mode=display">x_{w,l}=|\sum_{m=1}^M\sum_{n=1}^N S(X_{w,l}, A_{m,n}, \theta_{m,n})|</script><script type="math/tex; mode=display">S(X, A,\theta) = e^{J(h(X,A)-\theta)}</script><p>$ J $ : 虚数单位 </p>
<p>$ e^{Jθ} $ : 表示振幅为1的负指数</p>
<p>若网格X是初始位置，则理论相位 $ h(X, A) $和测量所得相位 $ \theta $ 相等。因此 $ h(X,A)-\theta $ 接近0，向量 $ e^{J(h(X,A)-\theta)} $ 在正方上接近实轴。所以，当 $ X $ 为标签所处位置时，$ S $ 值在网格 $ X $ 处叠加增强，产生较高的叠加 $ S $ 值。反之， $ h(X,A)-\theta $ 均匀分布在0-360度的范围内，$ S $ 值相加后相互抵消，该网格的 $ S $ 值较小。</p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_superimposing the observations_20200804.jpg" alt="research_rfid_tagoram_superimposing the observations_20200804"></p>
<ul>
<li><p>测试：A tag is interrogated 220 times by 2 antennas，结果如下图，标签实际位置 (108,68)</p>
<p>  <img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_naive hologram_20200804.jpg" alt="research_rfid_tagoram_naive hologram_20200804"></p>
</li>
</ul>
<h5 id="Augmented-Hologram-增广全息图"><a href="#Augmented-Hologram-增广全息图" class="headerlink" title="Augmented Hologram 增广全息图"></a>Augmented Hologram 增广全息图</h5><p>定义 $ \text{PSNR}=\frac{x_{w,l}}{\sum_{i=1}^W\sum_{j=1}^L x_{i,j}} $ ，实质就是归一化</p>
<script type="math/tex; mode=display">x_{w,l}=|\sum_{m=1}^M\sum_{n=1}^N ||S(X_{w,l}, A_{m,n}, \theta_{m,n})||S(X_{w,l}, A_{m,n}, \theta_{m,n})|</script><script type="math/tex; mode=display">\begin{cases}
||S(X, A, \theta)||=2\times F(|h(X, A)-\theta|;0,0.1) \\[2ex]
F(x ;\mu, \sigma)=\frac{1}{\sigma \sqrt{2\pi}}\int^{\infty}_{x} \text{exp } (-\frac{(t-\mu)^2}{2\sigma^2}) dt
\end{cases}</script><p>$ ||S|| $ : virtual amplitude 虚拟振幅。使用虚拟振幅将原来概率的各个分量乘以对应与相位差相关的因子，增大相位差小的分量，缩小相位差大的分量，使得图像中的初始位置更突出。</p>
<ul>
<li><p>测试：</p>
<p>  <img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_augmented hologram_20200804.jpg" alt="research_rfid_tagoram_augmented hologram_20200804"></p>
</li>
</ul>
<h5 id="Differential-Augmented-Hologram-差分增广全息图"><a href="#Differential-Augmented-Hologram-差分增广全息图" class="headerlink" title="Differential Augmented Hologram 差分增广全息图"></a>Differential Augmented Hologram 差分增广全息图</h5><script type="math/tex; mode=display">S(X, A,\theta) = e^{J(h(X,A)-(h(T,A)+c))}</script><script type="math/tex; mode=display">x_{m,l}=|\sum_{m=1}^M\sum_{n=1}^N ||\Bbb S(X_{w,l}, A_{m,n}, \theta_{m,n})||\Bbb S(X_{w,l}, A_{m,n}, \theta_{m,n})|</script><script type="math/tex; mode=display">\begin{cases}
\Bbb S(X_{w,l}, A_{m,n}, \theta_{m,n})=e^{J\theta_{dif}} \\[2ex] ||\Bbb S(X, A, \theta)||=2\times F(|\theta_{dif}|;0,0.1\times \sqrt 2) \\[2ex]
\theta_{dif}=(h(X_{w,l}, A_{m,n})-\theta_{m,n})-h(X_{w,l}, A_{m,1})-\theta_{m,1}))
\end{cases}</script><p>$(h(X_{w,l}, A_{m,n})-\theta_{m,n})-(h(X_{w,l}, A_{m,1})-\theta_{m,1})= \\<br>(h(X_{w,l}, A_{m,n})-(h(T, A_{m,n})+c)- \\   (h(X_{w,l}, A_{m,1})-(h(T, A_{m,1})+c))= \\<br>h(X_{w,l}, A_{m,n})-h(T, A_{m,n})+h(T, A_{m,1})-h(X_{w,l}, A_{m,1}) $</p>
<p>假设标签实际位置为 $ T $ ，当 $ T=X $ ，振幅应该最大，但是因为设备多样性导致的而偏移，加上一个 $ c $ 。每一个设备测得的相位差都被减去其中的第一个元素，通过差分消除各个设备包括相位偏移在内的系统误差。</p>
<ul>
<li><p>测试：</p>
<p>  <img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_differential augmented hologram_20200804.jpg" alt="research_rfid_tagoram_differential augmented hologram_20200804"></p>
</li>
</ul>
<h5 id="Real-time-track"><a href="#Real-time-track" class="headerlink" title="Real-time track"></a>Real-time track</h5><p>回顾之前的2D全息图，发现在全息图中，大多数像素的PSNR值较低（蓝色），而不需要进行相关计算。如果忽略这些像素，可以减少计算时间。当给定一个被测相位 $ \theta_{m,n} $ ，可以找到一组圆心在 $ A_{m,n} $ 的弧，称为候选网格，可以忽略其他网格来节省计算。使用哈希表实现了该想法。</p>
<p>假设天线初在原点 $ C_{m,n}=\{X|h(X,0)=\theta _{m,n}\} $</p>
<p>考虑噪声影响 $ C_{m,n}=\{X||h(X,0)-\theta_{m,n}| \le \sigma\} $ </p>
<p>加上天线的实际位置 $ C_{m,n}=\{X+\vec{A_{m,n}}||h(X,0)-\theta_{m,n}| \le \sigma\} $</p>
<h4 id="Movement-with-unknown-track"><a href="#Movement-with-unknown-track" class="headerlink" title="Movement with unknown track"></a>Movement with unknown track</h4><h5 id="Fitting-tag’s-trajectory-拟合轨迹"><a href="#Fitting-tag’s-trajectory-拟合轨迹" class="headerlink" title="Fitting tag’s trajectory 拟合轨迹"></a>Fitting tag’s trajectory 拟合轨迹</h5><p>传送带速度可达274.8mm/s，则标签运动速度可达9.16mm，远远小于设备频率对应波长。</p>
<p><strong>通过相邻两次读取的相位差估计标签的径向速度</strong></p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_modeling tag movement_20200804.jpg" alt="research_rfid_tagoram_modeling tag movement_20200804"></p>
<script type="math/tex; mode=display">\Delta d=\begin{cases}
\frac{\theta_{m,n+1}-\theta_{m,n}}{4\pi}\times \lambda, |\theta_{m,n+1}-\theta_{m,n}| < \pi \\[2ex] 
 \frac{(2\pi -\theta_{m,n+1}-\theta_{m,n})}{4\pi}\times \lambda, \theta_{m,n} - \theta_{m,n+1} \ge \pi \\[2ex]
 \frac{(-2\pi +\theta_{m,n+1}-\theta_{m,n})}{4\pi}\times \lambda, \theta_{m,n} - \theta_{m,n+1} \le \pi
\end{cases}</script><p>估计速度的大小和方向</p>
<script type="math/tex; mode=display">\tilde V_{m,n} \approx \frac{\Delta d}{t_{m, n+1} - t_{m,n}}</script><script type="math/tex; mode=display">\angle \tilde V_{m,n} \approx \angle(f(t_n)-A_m)</script><p>speed chain: </p>
<script type="math/tex; mode=display">|\vec V_{m,n}|=|\vec V_n|\text{ cos}(\angle\vec V_n-\angle\vec V_{m,n})</script><p>每轮找到 min $|V_{m,n}-\tilde V_{m,n}|$ </p>
<p>轨迹函数：</p>
<script type="math/tex; mode=display">f(t_n)=f(t_n-1)+f(t_n-t_{n-1})\cdot \vec V_n=f(t_0)+\sum_{k=1}^n(t_k-t_{k-1})\cdot \vec V_n</script><hr>
<h3 id="Implication-amp-Experiments"><a href="#Implication-amp-Experiments" class="headerlink" title="Implication &amp; Experiments"></a>Implication &amp; Experiments</h3><p>两种情况: controllable case  &amp; uncontrollable case，即可知运动轨道与速度和位置轨道</p>
<h4 id="Evaluation-in-controllable-case"><a href="#Evaluation-in-controllable-case" class="headerlink" title="Evaluation in controllable case"></a>Evaluation in controllable case</h4><ul>
<li>阅读器型号：Impinj Speedway R420，标签：Alien</li>
<li><p>speed of tag ：0.176m/s</p>
</li>
<li><p>Linear and circular track 直线和圆形的轨道</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_experiment setups_20200804.jpg" alt="research_rfid_tagoram_experiment setups_20200804"></p>
<h4 id="Accuracy-among-different-methods"><a href="#Accuracy-among-different-methods" class="headerlink" title="Accuracy among different methods"></a>Accuracy among different methods</h4><ul>
<li>Linear track 下测试，对比本文方法与之前的几种方法的精度</li>
</ul>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_accuracy comparison_20200804.jpg" alt="research_rfid_tagoram_accuracy comparison_20200804"></p>
<ul>
<li>RSS: 使用用RFID标签反向散射的RSS值（接收信号强度值），易受到多径效应，天线增益和标签与天线间角度的影响。</li>
<li>Otrack: 使用RSS和读取率read rate判断行李顺序</li>
<li>PinIt: 合成孔径雷达技术，SAR(reader motion)，主要参考标签</li>
<li>BackPos: 双曲定位技术（hyperbolic positioning）</li>
</ul>
<hr>
<h4 id="Tracking-in-circular-track"><a href="#Tracking-in-circular-track" class="headerlink" title="Tracking in circular track"></a>Tracking in circular track</h4><p>下图为圆形轨道测试的CDF（Cumulative Distribution Function）图</p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_tracking in circular track_20200804.jpg" alt="research_rfid_tagoram_tracking in circular track_20200804"></p>
<h4 id="Accuracy-among-different-holograms"><a href="#Accuracy-among-different-holograms" class="headerlink" title="Accuracy among different holograms"></a>Accuracy among different holograms</h4><ul>
<li><p>Linear track下测试</p>
</li>
<li><p>对NH, DH, DAH三种全息图建模进行了测试</p>
</li>
<li><p>NH的平均精度为600mm，标准偏差为100mm。AH克服了热噪声引起的偏差，降低了60%的偏差。DAH进一步消除了设备多样性的影响。它的第90百分位是18毫米，第99百分位是25毫米。</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_different holograms_20200804.jpg" alt="research_rfid_tagoram_different holograms_20200804"></p>
<h4 id="Real-time-performance"><a href="#Real-time-performance" class="headerlink" title="Real-time performance"></a>Real-time performance</h4><ul>
<li>计算量优化</li>
</ul>
<p>下图是生成全息图所需时间的CDF图，绿色的是阅读时间的概率分布曲线，在没有优化的情况下，$ 10^3 \times 10^3 $ 分辨率的全息图所消耗的时间中值为118ms，在每秒读取30次，读取间隔时间约为33ms的条件下间隔，计算时间远远超过这个读取间隔。使用哈希表进行优化的生成与读取时间相比总是保持较低的时间开销，即使分辨率达到 $ 10^5 \times 10^5 $ ，也能达到25ms的中值。</p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_read time_20200804.jpg" alt="research_rfid_tagoram_read time_20200804"></p>
<ul>
<li>实时性</li>
</ul>
<p>Tagoram需要在收集足够的读数后输出初始位置，那么需要多少次读取才能获得准确的结果？下描绘了准确性和实时性之间的关系。结果表明，当采集到120次以上的读数时，精度趋于稳定。因此，在接收到120个读取之后输出位置结果是合理的。读取120次总共需要120×25ms=2500ms。2.5秒的延迟对于对于机械系统的控制，如传送带或机械手臂的实时应用来说是可以接受的。</p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_stability_20200804.jpg" alt="research_rfid_tagoram_stability_20200804"></p>
<h4 id="Impacts-of-Parameters"><a href="#Impacts-of-Parameters" class="headerlink" title="Impacts of Parameters"></a>Impacts of Parameters</h4><p>验证其他的一些参数或设置不会影响实验结果，结果如下图，包括：</p>
<ul>
<li><p>频率 Frequency：对920-926MHZ的16个信道进行了测试。实际上，跳频功能（频率的自动变化）有助于改善阅读器和标签的连接</p>
</li>
<li><p>角度 Orientation：角度指标签与天线极化方向的夹角。</p>
</li>
<li><p>距离 Distance：标签到天线的距离。</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_impact of frequency_20200804.jpg" alt="research_rfid_tagoram_impact of frequency_20200804"></p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagotam_impact of orientation_2020804.jpg" alt="research_rfid_tagotam_impact of orientation_2020804"></p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_impact of distance_20200804.jpg" alt="research_rfid_tagoram_impact of distance_20200804"></p>
<h4 id="Evaluation-in-uncontrollable-case"><a href="#Evaluation-in-uncontrollable-case" class="headerlink" title="Evaluation in uncontrollable case"></a>Evaluation in uncontrollable case</h4><ul>
<li>4个天线，标签沿不规则轨道运动</li>
<li>收集了5分钟的数据，标签沿轨道逆时针运行10圈</li>
<li>测试平均速度 210.2mm/s，实际速度 212mm/s</li>
<li>结果表明在平缓的位置附近比在急转弯位置测试的结果要好（Has a better effect in the smooth part than the curve part），因为在拟合轨迹的时候是将每一段的运动当做匀速直线运动。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_top view of the track_20200804.jpg" alt="research_rfid_tagoram_top view of the track_20200804"></p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_estimated speed_20200804.jpg" alt="research_rfid_tagoram_estimated speed_20200804"></p>
<p><img src="https://raw.githubusercontent.com/agnes-yang/i/img/md_img/research_rfid_tagoram_fitted trajectory_20200804.jpg" alt="research_rfid_tagoram_fitted trajectory_20200804"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/03/Weakly-Supervised-Semantic-Segmentation-with-Boundary-Exploration/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="COOLA-LAB">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="COOLA-LAB">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/08/03/Weakly-Supervised-Semantic-Segmentation-with-Boundary-Exploration/" itemprop="url">Weakly Supervised Semantic Segmentation with Boundary Exploration</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-08-03T19:57:08+08:00">
                2020-08-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%AF%8F%E5%91%A8%E4%B8%80%E4%BC%9A/" itemprop="url" rel="index">
                    <span itemprop="name">每周一会</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-问题定位"><a href="#1-问题定位" class="headerlink" title="1 问题定位"></a>1 问题定位</h2><p>语义分割作为经典的计算机视觉的问题，从15年的FCN到之后的deeplab等，语义分割模型在自动驾驶，场景理解等场景都有较多的应该场景，同时也对神经网络的解释性产生一定的促进作用。对于输入的RGB图像或者灰度图，模型需要能够输出特定类别的区域，对于未设置的类别均作为背景处理。但是经典的语义分割模型都是使用像素级别的标签做监督，而这种标签在标注时非常耗时耗力，因为大家开始尝试使用弱监督(低于像素级别的监督)来完成语义分割任务，常见的弱监督包括：bounding box / scribble / image-level，本文也是基于image-level，图像级别的标签作为监督。该监督最为常见且监督级别最低，难度也较大。</p>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><p>大部分文章的解决思路是使用localization技术获取物体的初始位置信息，之后通过策略将初始定位结果进行优化。可以视为两阶段的localization-coarse。<br>localization方面，经典的技术包括saliency detection和CAM:</p>
<ul>
<li>CAM是使用全卷积网络提取特征，之后使用20个1x1的卷积核过滤，在将20张特征图通过GAP转换为20个标量值，使用多标签损失函数训练之后，20张特征图就对应20个类别的激活图。</li>
<li>saliency map是将分类模型的梯度反向传播回原图得到高激活值的区域即对应类别的位置。在反向传播的过程中，如果一个节点正向传播时没有激活，那么该节点不进行反向传播的梯度传递。<br>  coarse方面:</li>
<li>实现思路各式各样，不过主要要解决的问题是localization的定位面积过小，所以很多工作聚焦在鼓励类别区域扩撒，同时扩散的过程中保持边界性(由CRF完成)。</li>
</ul>
<h2 id="3-文章工作"><a href="#3-文章工作" class="headerlink" title="3 文章工作"></a>3 文章工作</h2><p>对于CAM进行了改进，提出了attention-pooling CAM。</p>
<ul>
<li>全卷积网络之后得到20张特征图$F_c$，对于每张$F_c$得各个位置，乘以参数$k$之后过一层softmax，得到attention map$A_c$。最终对应类别的置信度为特征图与attention map对应位置相乘后相加的值$P_c = \sum(F_c\times A_c)$</li>
<li>在梯度更新的反向传播时，attention map部分的梯度不进行传递。</li>
<li>训练收敛后的$F_c$即对应类别定位图。</li>
</ul>
<p>对于优化部分，目标是显式的预测类别的边界，通过规则将attention-pooling CAM的结果进行过滤，得到少量的标签正样本和较多的负样本以及无关的标签(不参与训练)。</p>
<ul>
<li>构造一个网络用于预测物体边界，属于边界二分类网络。训练数据使用从CAM合成得到的标签数据，损失函数方面由于样本的不均衡性所以做了归一化。如下<script type="math/tex; mode=display">L_{B}=-\sum_{i \in \Phi_{b r y}} \frac{W_{i} \log \left(P_{i}\right)}{\left|\Phi_{b r y}\right|}-\frac{1}{2}\left(\sum_{i \in \Phi_{c}} \frac{\log \left(1-P_{i}\right)}{\left|\Phi_{c}\right|}+\sum_{i \in \Phi_{b g}} \frac{\log \left(1-P_{i}\right)}{\left|\Phi_{b g}\right|}\right)</script></li>
<li>通过训练更多的类别边界得到挖掘，之后使用IRNet文章的工作，将边界图转换为亲和度矩阵，计算每个位置与其邻近像素之间的亲和度关系，作为概率转移矩阵，模拟随机游走扩散，对CAM的结果进行优化。</li>
</ul>
<p>经过优化之后，segmentation的结果就比较不错了，之后使用类似retrain的思路，将生成的语义分割结果和原图以全监督的方式训练deeplab模型，以得到最终的结果。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">&lt;i class&#x3D;&quot;fa fa-angle-right&quot;&gt;&lt;&#x2F;i&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">COOLA-LAB</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">COOLA-LAB</span>

  
</div>

<!--

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>




  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>



-->

        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
